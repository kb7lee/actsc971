

import numpy as np
import matplotlib.pyplot as plt
from sklearn import linear_model


class TMLSMC(object):
    
    def __init__( self, s1, r, sig1, T, M, sim, strike, degree, dispersion):
        self.s1 = s1
        self.r = r
        self.sig1 = sig1 
        self.T = T
        self.M = int(M)
        self.simulations = sim
        self.strike = strike
        self.time_unit = self.T / float(self.M)
        self.discount = np.exp(-self.r * self.time_unit)
        self.degree = degree
        self.dispersion = dispersion
        self.prediction = np.zeros([self.simulations, 1])
        self.coef = 0
        self.delta = np.zeros([self.simulations, 1])
        self.gamma = np.zeros([self.simulations, 1])
        self.payoff = np.zeros((self.M+1, self.simulations))
        self.value_matrix = np.zeros_like(self.payoff)
        
    def simulation(self, seed = 123):
        np.random.seed(seed)
        self.price1_matrix = np.zeros((self.M +1, self.simulations), dtype=np.float64)
        self.initial_values = self.s1 + np.random.standard_normal(self.simulations) * self.dispersion
        self.price1_matrix[0,:] = self.initial_values
        for t in range(1, self.M + 1):
            brownian1 = np.random.standard_normal( int(self.simulations / 2) )
            
            brownian1 = np.concatenate((brownian1, -brownian1))
            

            self.price1_matrix[t, :] = (self.price1_matrix[t - 1, :]
                                  * np.exp((self.r - self.sig1 ** 2 / 2) * self.time_unit
                                  + self.sig1 * brownian1 * np.sqrt(self.time_unit)))
            
        
    def cal_payoff(self):
        """ Designed for put. But we can easily switch to call """
        #self.payoff = np.maximum(self.strike - self.price1_matrix, np.zeros((self.M+1, self.simulations),dtype=np.float64))
        self.payoff = np.maximum(self.price1_matrix- self.strike, np.zeros((self.M+1, self.simulations),dtype=np.float64))
    
    
    def leastsquare(self, x,y):
       #x and y are both M*1 column vector
        x_nonzero=x[x>0]
        y_nonzero=y[x>0]
        x1=x_nonzero
        x2=x_nonzero**2
        x3=x_nonzero**3
        x4=x_nonzero**4
        x_mat=(np.vstack((x1,x2,x3,x4)).T)
        model=linear_model.LinearRegression(fit_intercept=True)
        model.fit(x_mat,y_nonzero)
        #self.coef = np.append(model.intercept_, model.coef_)
        x_pre=np.vstack((x,x**2,x**3,x**4)).T
        y_pre=model.predict(x_pre).reshape(example.simulations,1)[:,0]
        model_prediction=np.where(x>0,y_pre,0)
        self.coef = np.append(model.intercept_, model.coef_)
        self.prediction = model_prediction
    
    def ls_calculation(self):
        self.value_matrix[-1, :] = self.payoff[-1, :]
        for t in range(self.M - 1, 0 , -1):
            choice = self.payoff[t,:] > 0
            X = np.where(choice, self.price1_matrix[t,:], 0)
            Y = np.where(choice, self.value_matrix[t + 1, :] * self.discount, 0)
            self.leastsquare(X,Y)
            continuation_value = np.transpose(self.prediction)
            self.value_matrix[t, :] = np.where(self.payoff[t, :] > continuation_value, self.payoff[t, :], 
                            self.value_matrix[t + 1, :] * self.discount)
        
        Y = self.value_matrix[1, :]
        X = self.initial_values
        self.leastsquare(X,Y)
        
        return np.mean(self.prediction)

    def calculate_delta(self):
        self.S_mat = np.vstack((np.zeros(len(self.initial_values)),
                                np.ones(len(self.initial_values)),
                                2*self.initial_values,
                                3*self.initial_values **2, 
                                4*self.initial_values **3))
        self.delta = np.matmul(self.coef, self.S_mat)
        return np.mean(self.delta)
    
    def calculate_gamma(self):
        self.S_mat = np.vstack((np.zeros(len(self.initial_values)),
                                np.zeros(len(self.initial_values)),
                                2*np.ones(len(self.initial_values)),
                                3*2*self.initial_values , 
                                4*3*self.initial_values **2))
        self.gamma = np.matmul(self.coef, self.S_mat)
        return np.mean(self.gamma)
            
    

    
    
example = TMLSMC(36, 0.06, 0.2, 1, 50, 10000, 40, 3, 0.9)
example.simulation()
example.cal_payoff()
print(example.ls_calculation())
print(example.calculate_delta())
print(example.calculate_gamma())

