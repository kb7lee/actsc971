

# American Put option

import numpy as np
import matplotlib.pyplot as plt
from sklearn import linear_model


class MLSMC(object):
    
    def __init__( self, s1, r, sig1, T, M, sim, strike, degree, dispersion):
        self.s1 = s1
        self.r = r
        self.sig1 = sig1 
        self.T = T
        self.M = int(M)
        self.simulations = sim
        self.strike = strike
        self.time_unit = self.T / float(self.M)
        self.discount = np.exp(-self.r * self.time_unit)
        self.degree = degree
        self.dispersion = dispersion
        self.prediction = np.zeros([self.simulations, 1])
        self.coef = 0
        self.delta = np.zeros([self.simulations, 1])
        self.gamma = np.zeros([self.simulations, 1])
        
    def simulation(self, seed = 12992323):
        np.random.seed(seed)
        self.price1_matrix = np.zeros((self.M + 1, self.simulations), dtype=np.float64)
        self.initial_values = self.s1 + np.random.standard_normal(self.simulations) * self.dispersion
        self.price1_matrix[0,:] = self.initial_values
        for t in range(1, self.M + 1):
            brownian1 = np.random.standard_normal( int(self.simulations / 2) )
            
            brownian1 = np.concatenate((brownian1, -brownian1))
            

            self.price1_matrix[t, :] = (self.price1_matrix[t - 1, :]
                                  * np.exp((self.r - self.sig1 ** 2 / 2) * self.time_unit
                                  + self.sig1 * brownian1 * np.sqrt(self.time_unit)))
            
        
    def put_payoff(self):
        """ Designed for put. But we can easily switch to call """
        self.payoff = np.maximum(self.strike - self.price1_matrix, np.zeros((self.M+1, self.simulations),dtype=np.float64))
    
#    def ls_regression(self, X, Y):
#        X_mat = X
#        for i in range(2, 3+1):
#            X_mat = np.append(X_mat, X ** i, axis = 0)
#     
#        model = linear_model.LinearRegression()
#        model.fit(np.transpose(X_mat), np.transpose(Y))
#        self.coef = [model.intercept_, model.coef_[0]]
#
#        for i in range(0,X.shape[1]):
#            self.predicted[i,0] = sum(self.coef[1:][0] * X_mat[:,i]) + self.coef[0]
        
    def leastsquare(self, x,y):
       #x and y are both M*1 column vector
        x_nonzero=x[x>0]
        y_nonzero=y[x>0]
        x1=x_nonzero
        x2=x_nonzero**2
        x3=x_nonzero**3
        x_mat=np.vstack((x1,x2,x3)).T
        model=linear_model.LinearRegression(fit_intercept=True)
        model.fit(x_mat,y_nonzero)
        model_prediction=np.zeros((self.simulations,1))
        self.coef = np.append(model.intercept_, model.coef_)
        for i in range(0,self.simulations):
            if x[i]==0:
                continue
            else:
                a=x[i]
                a1=a
                a2=a**2
                a3=a**3
                a_mat=np.hstack((a1,a2,a3))
                model_prediction[i,0]=model.predict(a_mat.reshape(1, -1))
       
        self.prediction = model_prediction[:,0]
    
    def ls_calculation(self):
        self.value_matrix = np.zeros_like(self.payoff)
        self.value_matrix[-1, :] = self.payoff[-1, :]
        for t in range(self.M - 1, 0 , -1):
            choice = self.payoff[t,:] > 0
            X = np.where(choice, self.price1_matrix[t,:], 0)
            Y = np.where(choice, self.value_matrix[t + 1, :] * self.discount, 0)
            self.leastsquare(X,Y)
            continuation_value = np.transpose(self.prediction)
            self.value_matrix[t, :] = np.where(self.payoff[t, :] > continuation_value, self.payoff[t, :], 
                            self.value_matrix[t + 1, :] * self.discount)
        
        Y = self.value_matrix[1, :]
        X = self.initial_values
        self.leastsquare(X,Y)
        
        return np.mean(self.prediction)

    def calculate_delta(self):
        
        for i in range(0,self.simulations):
            x = self.initial_values[i]
            x0 = 0 
            x1=1
            x2=2*x
            x3=3*x**2
            x_mat=np.vstack((x0, x1,x2,x3)).T
            self.delta[i,0] = np.sum(x_mat * self.coef)
        return np.mean(self.delta)
    
    def calculate_gamma(self):
        
        for i in range(0,self.simulations):
            x = self.initial_values[i]
            x0 =0 
            x1=0
            x2=2
            x3=3*2*x
            x_mat=np.vstack((x0,x1,x2,x3)).T
            self.gamma[i,0] = np.sum(x_mat * self.coef)
        return np.mean(self.gamma)
            
    
    
example = MLSMC(40, 0.0488, 0.2, 7/12, 50, 10000, 40, 3, 1.5)
example.simulation()
example.put_payoff()
print(example.ls_calculation())
print(example.calculate_delta())
print(example.calculate_gamma())
